[model]
kernel_sizes = (8, 10, 64), (1, 6, 155), (9, 7, 267), (2, 6, 348)
factor_sched = 2
dropout2d_on_conv = False
flatten_style = keep_both
time_factor = 1
freq_factor = 4
hidden_sizes = 1198, 582

[training]
num_epochs = 100

[data]
context_left = 20
context_right = 12

[optim]
initial_design_samples = 50
to_optimize = reduce_lr_log10_epsilon, early_stopping_burnin, reduce_lr_factor, early_stopping_patience, log10_learning_rate, reduce_lr_threshold, reduce_lr_burnin, weight_decay, early_stopping_threshold, reduce_lr_cooldown, weigh_training_samples, reduce_lr_patience, dropout_prob, optimizer, batch_size
partition_style = last
median_pruner_epoch_warmup = 5
sampler = tpe
