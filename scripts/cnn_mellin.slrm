#! /usr/bin/env bash
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=25G
#SBATCH --export=ALL
#SBATCH --output=exp/logs/slurm-%J.log
#SBATCH --open-mode=append
#SBATCH --gres=gpu:1
#SBATCH --time=infinite

model_dir=
slurm_model_dir="/checkpoint/sdrobert/${SLURM_JOB_ID}"

if [ -z "${SLURM_JOB_ID}" ]; then
  log_file="exp/logs/cnn_mellin.$(date +%s)"
  echo "Looks like we aren't running slurm. Forking output to ${log_file}.log"
  mkdir -p exp/logs
  exec > >(tee -a "${log_file}.log")
else
  model_dir="${slurm_model_dir}"
fi

echo "$0 called with arguments: $*"

# first determine if we've got cnn-mellin installed
if ! which cnn-mellin > /dev/null ; then
  echo -e "$0: cnn-mellin unavailable. Did you run 'pip install -e .'?"
  exit 1
fi

args=( "$@" )

# set model flag if we have it
if ! [[ "${args[*]}" =~ --model-dir ]] && ! [ -z "${model_dir}" ]; then
  model_flag="--model-dir=${slurm_model_dir}"
  echo "Could not find --model-dir flag. Adding '${model_flag}'"
  args+=( "$model_flag" )
fi

# set device flag if we have it
if ! [[ "${args[*]}" =~ --device ]]; then
  device_flag="--device=$( python -c 'import torch; print("cuda" if torch.cuda.is_available() else "cpu")')"
  echo "Could not find --device flag. Adding '$device_flag'"
  args=( "$device_flag" "${args[@]}" )
fi
if [[ "${args[*]}" =~ --device=cuda ]]; then
  nvidia-smi
fi

# this loop is primarily for optim when an estimate caused an OOM exception.
for x in $(seq 1 100); do
  cnn-mellin "${args[@]}"
  echo -e "Call ended with error (attempt $x/100)"
done
