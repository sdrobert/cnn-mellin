#! /usr/bin/env bash
#SBATCH --wait
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=25G
#SBATCH --export=ALL
#SBATCH --output=exp/logs/slurm-%J.log
#SBATCH --gres=gpu:1

source scripts/preamble.slrm

args=( "$@" )

# set model flag if we have it
if ! [[ "${args[*]}" =~ --model-dir ]]; then
  model_flag="--model-dir=${temp_dir}/models"
  echo "Could not find --model-dir flag. Adding '${model_flag}'"
  args+=( "$model_flag" )
fi

# set device flag if we have it
if ! [[ "${args[*]}" =~ --device ]]; then
  device_flag="--device=$( python -c 'import torch; print("cuda" if torch.cuda.is_available() else "cpu")')"
  echo "Could not find --device flag. Adding '$device_flag'"
  args=( "$device_flag" "${args[@]}" )
fi
if [[ "${args[*]}" =~ --device=cuda ]]; then
  if [ -z "$CUDA_HOME" ]; then
    echo -e "CUDA_HOME unset! Needed for compilation!"
  fi
  nvidia-smi || true
fi

# Make sure we can compile the extensions
python -c 'import warnings; warnings.simplefilter("error"); import mconv' || exit 1

# this loop is primarily for optim when an estimate caused an OOM exception.
for x in $(seq 1 100); do
  if [[ " ${args[*]} " =~ " optim " ]]; then
    echo "Sleeping for 120 secs to wear out heartbeat"
    sleep 120
  fi
  echo "Calling (attempt $x/100)"
  python asr.py "${args[@]}" && exit 0
  echo -e "Call ended with error (attempt $x/100)"
done
